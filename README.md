# simple_seq2seq-GRU-
RNN의 단점을 보완한 GRU에 대해 알아보기 위해 시작한 간단한 프로젝트
아스키 코드를 데이터 셋으로 사용해 hello를 hola로 번역한다.

# RNN의 문제가 뭘까?
사람이 긴 문장을 읽다보면 앞부분의 맥락이 기억나지 않는 것처럼
RNN을 이용한 딥러닝 모델도 문장의 뒷 부분으로 갈 수록 앞부분의 정보가 소실된다.

이뿐만 아니라, RNN은 입력이 길어지면 학습 도중 기울기가 너무 작아지거나 커져서 앞부분에 대한 정보를 정확히 담지 못한다.
기울기가 RNN 학습 도중 폭발적으로 커지는 현상을 gradient explosion과
기울기가 너무 작아지는 현상을 vanishing gradient를 모두 겪을 수 있다.

그러나 GRU를 사용해 시계열 데이터 내 정보 전달량을 조절하면 이런 단점을 해결할 수 있다.

# 그럼 GRU는 뭔데?
시계열 데이터 내 정보 전달량을 조절하는 업데이트 게이트와 리셋 게이트를 사용.
이전 은닉 벡터가 지닌 정보를 새로운 는닉 벡터가 얼마나 유지할지 정해주는 역할을 업데이트 게이트,
새로운 입력이 이전 은닉 벡터와 어떻게 조합할지는 리셋 게이트가 결정한다.

이 둘을 적절히 사용해서 벡터 사이의 정보 전달량을 조절하고 기울기를 적정하게 유지함으로써 문장 앞부분의 정보가 끝까지 도달할 수 있도록 도와준다.

# 환경
- python = 3.x
- pytorch = 1.18

# 데이터셋
## 아스키 코드
- IMDB는 다수의 영어문장으로 구성되어 있으며, 5만건의 영화리뷰 데이터가 존재함.
- positive review = 2 / negative review = 1 로 레이블링 돼있음

# 목표
- 영어 hello를 hola로 번역하는 아주 간단한 기계 번역 모델을 구현한다.
